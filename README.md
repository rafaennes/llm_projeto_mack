# LLM Projeto - TransparÃªncia Governamental (MCP Agent)

Uma aplicaÃ§Ã£o de **Agente Inteligente** para transparÃªncia governamental. O sistema utiliza a arquitetura **MCP (Model Context Protocol)** onde um Agente (Streamlit + Qwen) orquestra o uso de ferramentas para responder perguntas tanto tÃ©cnicas quanto teÃ³ricas sobre emendas parlamentares.

## ğŸ—ï¸ Arquitetura

O projeto segue estritamente o protocolo MCP, separando a inteligÃªncia (Cliente/Agente) das ferramentas (Servidor).

```mermaid
graph TD
    User([ğŸ‘¤ UsuÃ¡rio]) <--> Client[ğŸ–¥ï¸ MCP Client (Streamlit + Agente Qwen)]
    Client <-->|Stdio / Pipe| Server[âš™ï¸ MCP Server (Python)]
    
    subgraph "Agente de DecisÃ£o"
        Client -- "Analisa Pergunta" --> Intent{RaciocÃ­nio}
        Intent -- "Dados?" --> ToolSql[ğŸ› ï¸ query_emendas]
        Intent -- "Teoria?" --> ToolDocs[ğŸ› ï¸ search_legislative_report]
    end

    subgraph "MCP Tools"
        ToolSql --> SQLite[(ğŸ’¾ Banco de Dados)]
        ToolDocs --> Markdown[(ğŸ“„ RelatÃ³rios/Leis)]
    end
```

## âœ¨ Funcionalidades

### ğŸ¤– Agente Qwen Integrado
O cliente de chat nÃ£o Ã© apenas uma interface, mas um **Agente AutÃ´nomo** rodando o modelo `Qwen-2.5-1.5B`. Ele:
1.  **Entende** a pergunta do usuÃ¡rio.
2.  **Decide** qual ferramenta usar:
    *   *Dados quantitativos* -> Gera SQL e consulta o banco.
    *   *DÃºvidas teÃ³ricas* -> Pesquisa na base de conhecimento (Markdown).
3.  **Responde** de forma natural com base no retorno das ferramentas.

### ğŸ› ï¸ Ferramentas MCP DisponÃ­veis
O servidor expÃµe as seguintes capabilities:

| Ferramenta | DescriÃ§Ã£o | Exemplo de Uso |
|------------|-----------|----------------|
| `query_emendas` | Executa consultas SQL seguras na base de dados. | "Quanto foi pago para SP?" |
| `search_legislative_report` | Busca semÃ¢ntica/keyword em documentos teÃ³ricos. | "O que Ã© uma emenda pix?" |
| `get_emendas_schema` | Retorna a estrutura da tabela. | "Quais campos existem?" |
| `get_emendas_stats` | EstatÃ­sticas gerais do banco. | "Resumo dos dados" |

## ğŸš€ Como Rodar (Deploy)

### PrÃ©-requisitos
- Python 3.10+
- 4GB+ RAM (para rodar o modelo LLM local)

### 1. Setup Inicial
Prepare o ambiente e o banco de dados:

```bash
# 1. Instalar dependÃªncias
./local_deploy/setup_env.sh

# 2. Popular o banco SQLite (se ainda nÃ£o fez)
source venv/bin/activate
python local_deploy/init_sqlite.py
```

### 2. Rodar a AplicaÃ§Ã£o
Basta iniciar o cliente. Ele cuidarÃ¡ de subir o servidor MCP automaticamente via Stdio.

```bash
./mcp_client/start_chat.sh
```

Acesse em: `http://localhost:8501`

![Interface do Agente](https://raw.githubusercontent.com/modelcontextprotocol/assets/main/demo.png) *(Imagem ilustrativa)*

## ğŸ“‚ Estrutura de Pastas

*   `mcp_client/`: **Agente Inteligente**. ContÃ©m a lÃ³gica do Chat e integraÃ§Ã£o com LLM.
    *   `chat_app.py`: CÃ³digo principal do Agente.
*   `mcp_server/`: **Provedor de Ferramentas**. NÃ£o tem IA, apenas executa funÃ§Ãµes.
    *   `server.py`: DefiniÃ§Ã£o das ferramentas MCP.
*   `data/`:
    *   `db_transparencia.db`: Banco SQLite.
    *   `dicionario_dados.md`: DicionÃ¡rio de dados para contexto do Agente.
    *   `teorico/`: Documentos para busca textual.

## ğŸ“š Stack TecnolÃ³gica
- **Protocolo**: Model Context Protocol (MCP)
- **Frontend**: Streamlit
- **LLM**: Qwen-2.5-1.5B (GGUF via `llama-cpp-python`)
- **Database**: SQLite3
- **Search**: Busca textual simples em Markdown
# llm_projeto_mack
