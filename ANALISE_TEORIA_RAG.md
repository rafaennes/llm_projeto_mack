# ğŸ“š AnÃ¡lise: Perguntas TeÃ³ricas - Busca Simples vs RAG

## ğŸ” Problema Atual

### Fluxo Atual (Busca Keyword Simples)

```
UsuÃ¡rio: "O que Ã© emenda PIX?"
    â†“
1. MCP Server: search_in_markdown("O que Ã© emenda PIX?")
    â†“
2. Busca: query_terms = ["o", "que", "Ã©", "emenda", "pix"]
    â†“
3. Retorna: Top 5 parÃ¡grafos que contÃªm QUALQUER termo
    â†“
4. LLM Qwen: Recebe ~5000 chars de texto + prompt
    â†“
5. Problema: Contexto pode ser irrelevante! âŒ
```

### Problemas Identificados

1. **Busca muito superficial**
   - Apenas keyword matching (`"pix" in texto.lower()`)
   - Retorna parÃ¡grafos com ANY palavra, nÃ£o ALL
   - Sem ranking de relevÃ¢ncia

2. **Sem compreensÃ£o semÃ¢ntica**
   - "emenda PIX" vs "emenda de relator" sÃ£o tratados igual
   - NÃ£o entende sinÃ´nimos ou contexto

3. **LLM pequeno (Qwen 1.5B) sem contexto suficiente**
   - Modelo pequeno tem dificuldade de interpretar texto longo
   - Tende a "alucinaÃ§Ã£o" quando contexto Ã© confuso

4. **Qualidade do contexto recuperado**
   - Top 5 parÃ¡grafos podem nÃ£o ser os mais relevantes
   - Pode faltar informaÃ§Ã£o chave

---

## ğŸ¯ SoluÃ§Ãµes Propostas

### OpÃ§Ã£o 1: **Busca BM25 (Mais RÃ¡pida)** âš¡

**O que Ã©:** Algoritmo de ranking estatÃ­stico (TF-IDF melhorado)

**Vantagens:**
- âœ… Muito mais rÃ¡pido que embeddings (~50ms)
- âœ… NÃ£o precisa de GPU
- âœ… Rank melhor que keyword simples
- âœ… Biblioteca Python simples (`rank-bm25`)

**Desvantagens:**
- âš ï¸ Ainda nÃ£o Ã© semÃ¢ntico (nÃ£o entende sinÃ´nimos)
- âš ï¸ Precisa de palavras exatas

**ImplementaÃ§Ã£o:**
```python
from rank_bm25 import BM25Okapi

# PrÃ©-processa uma vez
paragraphs = [p for p in doc.split('\n\n')]
tokenized_corpus = [p.lower().split() for p in paragraphs]
bm25 = BM25Okapi(tokenized_corpus)

# Busca
query_tokens = user_question.lower().split()
scores = bm25.get_scores(query_tokens)
top_indices = np.argsort(scores)[-5:]  # Top 5
```

**Resultado esperado:**
- Velocidade: ~50ms (vs ~30ms atual)
- Qualidade: +30-40% melhor ranking

---

### OpÃ§Ã£o 2: **RAG com Embeddings (Melhor Qualidade)** ğŸ¯

**O que Ã©:** VetorizaÃ§Ã£o semÃ¢ntica + busca por similaridade

**Vantagens:**
- âœ… Busca semÃ¢ntica real (entende sinÃ´nimos, contexto)
- âœ… "emenda PIX" encontra "transferÃªncia especial"
- âœ… Qualidade muito superior
- âœ… Pode usar modelo pequeno de embedding (~100MB)

**Desvantagens:**
- âŒ Mais lento: ~200-500ms para embedding + busca
- âŒ Precisa vetorizar documentos (setup inicial)
- âŒ Mais complexo

**Stack Sugerida:**
```python
# Modelo de embedding leve
from sentence_transformers import SentenceTransformer

# OpÃ§Ã£o 1: all-MiniLM-L6-v2 (80MB, rÃ¡pido)
model = SentenceTransformer('all-MiniLM-L6-v2')

# OpÃ§Ã£o 2: Modelo em portuguÃªs
model = SentenceTransformer('neuralmind/bert-base-portuguese-cased')

# Vector store simples
import faiss
index = faiss.IndexFlatL2(384)  # DimensÃ£o do embedding
```

**Resultado esperado:**
- Velocidade: +200-300ms (ainda aceitÃ¡vel)
- Qualidade: +60-80% melhor relevÃ¢ncia

---

### OpÃ§Ã£o 3: **HÃ­brido BM25 + Reranking** âš¡ğŸ¯ **(RECOMENDADO)**

**Como funciona:**
1. BM25 recupera top 20 candidatos (~50ms)
2. Reranker classifica os 20 (~100ms)
3. Retorna top 5 realmente relevantes

**Vantagens:**
- âœ… Velocidade boa (~150ms total)
- âœ… Qualidade prÃ³xima de RAG puro
- âœ… Melhor custo-benefÃ­cio

**ImplementaÃ§Ã£o:**
```python
from rank_bm25 import BM25Okapi
from sentence_transformers import CrossEncoder

# Stage 1: BM25 (rÃ¡pido, recupera 20)
bm25_results = bm25.get_top_n(query, paragraphs, n=20)

# Stage 2: Reranker (preciso, classifica 20)
reranker = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')
scores = reranker.predict([(query, doc) for doc in bm25_results])
top_5 = [bm25_results[i] for i in np.argsort(scores)[-5:]]
```

---

## ğŸ“Š ComparaÃ§Ã£o das OpÃ§Ãµes

| CritÃ©rio | Keyword Atual | BM25 | RAG Embeddings | HÃ­brido BM25+Rerank |
|----------|--------------|------|----------------|---------------------|
| **Velocidade** | ~30ms | ~50ms | ~300ms | ~150ms |
| **Qualidade** | â­â­ | â­â­â­ | â­â­â­â­â­ | â­â­â­â­ |
| **Complexidade** | Baixa | Baixa | Alta | MÃ©dia |
| **Uso RAM** | ~5MB | ~10MB | ~200MB | ~150MB |
| **GPU necessÃ¡ria?** | NÃ£o | NÃ£o | NÃ£o* | NÃ£o* |
| **Setup inicial** | 0s | ~1s | ~30s | ~5s |

*Modelos pequenos rodam bem em CPU

---

## ğŸ¯ Minha RecomendaÃ§Ã£o

### Para SEU caso especÃ­fico: **OpÃ§Ã£o 3 (HÃ­brido BM25 + Reranker)**

**Justificativa:**

1. **Documento pequeno (42KB)**
   - RAG puro seria "overkill"
   - HÃ­brido Ã© suficiente

2. **Qwen 1.5B jÃ¡ estÃ¡ no limite**
   - Precisa de contexto BEM selecionado
   - Qualidade do retrieval Ã© CRÃTICA

3. **UsuÃ¡rio quer velocidade**
   - 150ms adicional Ã© aceitÃ¡vel
   - Muito melhor que 300ms do RAG puro

4. **NÃ£o precisa de GPU**
   - Reranker roda bem em CPU
   - NÃ£o adiciona dependÃªncia de hardware

---

## ğŸ› ï¸ ImplementaÃ§Ã£o Sugerida

### Estrutura Proposta

```
mcp_server/
â”œâ”€â”€ server.py
â””â”€â”€ retrieval/
    â”œâ”€â”€ __init__.py
    â”œâ”€â”€ bm25_index.py      # BM25 indexing
    â”œâ”€â”€ reranker.py        # Cross-encoder reranker
    â””â”€â”€ hybrid_search.py   # OrquestraÃ§Ã£o
```

### CÃ³digo Base

#### 1. `retrieval/bm25_index.py`

```python
from rank_bm25 import BM25Okapi
import pickle
import os

class BM25Index:
    def __init__(self, documents):
        """documents: lista de strings (parÃ¡grafos)"""
        self.documents = documents
        tokenized = [doc.lower().split() for doc in documents]
        self.bm25 = BM25Okapi(tokenized)

    def search(self, query, top_k=20):
        """Retorna top_k documentos mais relevantes"""
        query_tokens = query.lower().split()
        scores = self.bm25.get_scores(query_tokens)
        top_indices = sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)[:top_k]
        return [(self.documents[i], scores[i]) for i in top_indices]

    def save(self, path):
        with open(path, 'wb') as f:
            pickle.dump((self.documents, self.bm25), f)

    @classmethod
    def load(cls, path):
        with open(path, 'rb') as f:
            documents, bm25 = pickle.load(f)
        obj = cls.__new__(cls)
        obj.documents = documents
        obj.bm25 = bm25
        return obj
```

#### 2. `retrieval/reranker.py`

```python
from sentence_transformers import CrossEncoder
import numpy as np

class Reranker:
    def __init__(self, model_name='cross-encoder/ms-marco-MiniLM-L-6-v2'):
        """Reranker usando cross-encoder"""
        self.model = CrossEncoder(model_name)

    def rerank(self, query, documents, top_k=5):
        """Reordena documentos por relevÃ¢ncia"""
        if len(documents) == 0:
            return []

        # Cria pares (query, doc)
        pairs = [(query, doc) for doc in documents]

        # Calcula scores
        scores = self.model.predict(pairs)

        # Retorna top_k
        top_indices = np.argsort(scores)[-top_k:][::-1]
        return [(documents[i], scores[i]) for i in top_indices]
```

#### 3. `retrieval/hybrid_search.py`

```python
from .bm25_index import BM25Index
from .reranker import Reranker
import os

class HybridSearch:
    def __init__(self, index_path=None):
        self.bm25 = None
        self.reranker = Reranker()

        if index_path and os.path.exists(index_path):
            self.bm25 = BM25Index.load(index_path)

    def index_documents(self, markdown_file, index_path):
        """Indexa um arquivo markdown"""
        with open(markdown_file, 'r', encoding='utf-8') as f:
            content = f.read()

        # Divide em parÃ¡grafos
        paragraphs = [p.strip() for p in content.split('\n\n') if len(p.strip()) > 50]

        # Cria Ã­ndice BM25
        self.bm25 = BM25Index(paragraphs)
        self.bm25.save(index_path)

        print(f"âœ… Indexed {len(paragraphs)} paragraphs")

    def search(self, query, top_k=5):
        """Busca hÃ­brida: BM25 + Reranking"""
        if not self.bm25:
            return []

        # Stage 1: BM25 recupera 20 candidatos
        candidates = self.bm25.search(query, top_k=20)
        docs_only = [doc for doc, score in candidates]

        # Stage 2: Reranker seleciona top 5
        reranked = self.reranker.rerank(query, docs_only, top_k=top_k)

        return [doc for doc, score in reranked]
```

#### 4. IntegraÃ§Ã£o no `server.py`

```python
# No inÃ­cio do arquivo
from retrieval.hybrid_search import HybridSearch
import os

# VariÃ¡vel global
hybrid_search = None

def get_hybrid_search():
    """Inicializa busca hÃ­brida (lazy loading)"""
    global hybrid_search
    if hybrid_search is None:
        index_path = os.path.join(os.path.dirname(__file__), 'retrieval', 'bm25_index.pkl')
        hybrid_search = HybridSearch(index_path)

        # Se Ã­ndice nÃ£o existe, cria
        if not os.path.exists(index_path):
            md_file = os.path.join(os.path.dirname(__file__), '..', 'data', 'teorico', 'Relatorio_Emendas_Parlamentares.md')
            hybrid_search.index_documents(md_file, index_path)

    return hybrid_search

# Substitui search_in_markdown
def search_in_markdown(query: str) -> str:
    """Busca hÃ­brida BM25 + Reranking"""
    try:
        searcher = get_hybrid_search()
        results = searcher.search(query, top_k=5)

        if not results:
            return "Nenhum resultado encontrado nos documentos teÃ³ricos."

        # Formata resultados
        formatted = []
        for i, doc in enumerate(results, 1):
            formatted.append(f"**Trecho {i}:**\n{doc}\n")

        return "\n---\n".join(formatted)

    except Exception as e:
        return f"Erro na busca: {str(e)}"
```

---

## ğŸ“¦ DependÃªncias Adicionais

```bash
pip install rank-bm25 sentence-transformers
```

**Tamanho adicional:**
- rank-bm25: ~50KB
- sentence-transformers: ~5MB
- Modelo reranker: ~80MB (download automÃ¡tico)

**Total:** ~85MB adicional

---

## âš¡ Performance Esperada

### Antes (Keyword simples):
```
Pergunta: "O que Ã© emenda PIX?"
Tempo: 8-12s total
â”œâ”€ Busca: 30ms
â”œâ”€ LLM: 8-12s
â””â”€ Qualidade: â­â­ (contexto ruim)
```

### Depois (HÃ­brido BM25 + Rerank):
```
Pergunta: "O que Ã© emenda PIX?"
Tempo: 8-12s total (similar!)
â”œâ”€ Busca: 150ms (+120ms)
â”œâ”€ LLM: 8-12s (mesmo tempo, mas contexto MELHOR)
â””â”€ Qualidade: â­â­â­â­ (contexto relevante)
```

**Resultado:** Mesma velocidade, MUITO mais qualidade!

---

## ğŸ¯ DecisÃ£o Final

### Eu recomendo: **Implementar HÃ­brido (OpÃ§Ã£o 3)**

**Motivos:**
1. âœ… Melhor custo-benefÃ­cio
2. âœ… NÃ£o aumenta tempo perceptÃ­vel (~150ms Ã© insignificante vs 8s do LLM)
3. âœ… Qualidade MUITO superior
4. âœ… FÃ¡cil de implementar (~200 linhas)
5. âœ… NÃ£o precisa de GPU

### Se nÃ£o quiser complexidade: **BM25 simples (OpÃ§Ã£o 1)**

**Motivos:**
1. âœ… Muito simples (~50 linhas)
2. âœ… +20ms apenas
3. âœ… JÃ¡ melhora 30-40%
4. âŒ NÃ£o Ã© semÃ¢ntico

---

## ğŸ§ª Plano de ImplementaÃ§Ã£o

### Fase 1: Proof of Concept (1-2h)
1. Instalar dependÃªncias
2. Criar script de teste isolado
3. Comparar resultados: keyword vs BM25 vs hÃ­brido

### Fase 2: IntegraÃ§Ã£o (1-2h)
1. Criar mÃ³dulo `retrieval/`
2. Modificar `server.py`
3. Testar com queries reais

### Fase 3: ValidaÃ§Ã£o (30min)
1. Criar benchmark de queries
2. Medir qualidade (manual)
3. Medir velocidade

---

## ğŸ“ PrÃ³ximos Passos

Quer que eu:
1. âœ… Implemente a soluÃ§Ã£o hÃ­brida completa?
2. âœ… Crie script de teste para comparar abordagens?
3. âœ… Implemente apenas BM25 simples primeiro?

**Minha recomendaÃ§Ã£o:** ComeÃ§ar com **script de teste** para vocÃª ver a diferenÃ§a de qualidade, depois decidir qual implementar.

---

**Autor:** Claude (2025-12-09)
